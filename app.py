import pickle, faiss, numpy as np, streamlit as st
from openai import OpenAI

VEC_PATH = "data/index.faiss"
META_PATH = "data/meta.pkl"

st.set_page_config(page_title="RAGæ©Ÿèƒ½ã‚¢ãƒ—ãƒª & ã¾ã¨ã‚ Demo")
st.title("ğŸ“„ RAGæ©Ÿèƒ½ã‚¢ãƒ—ãƒª / ã¾ã¨ã‚")

@st.cache_resource
def load_resources():
    index = faiss.read_index(VEC_PATH)
    metas = pickle.load(open(META_PATH, "rb"))
    client = OpenAI()
    return index, metas, client
index, metas, client = load_resources()

query = st.text_input("è³ªå•ï¼š", "")
topk = st.slider("æœ€åˆã®æ•°ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¡¨ç¤º", 1, 5, 3)
if query:
    q_emb = client.embeddings.create(
        model="text-embedding-3-small",
        input=[query]
    ).data[0].embedding
    D,I = index.search(np.array([q_emb]).astype("float32"), topk)
    for rank, idx in enumerate(I[0], 1):
        fname, chunk = metas[idx]
        with st.expander(f"#{rank}ã€€{fname}"):
            # LLM ã¾ã¨ã‚
            summ = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role":"user",
                     "content":f"150å­—å…§ã®æ—¥æœ¬èªã§ã¾ã¨ã‚ã¦ãã ã•ã„ï¼š\n\n{chunk}"}
                ]
            ).choices[0].message.content
            st.markdown("**ã¾ã¨ã‚ï¼š** " + summ)
            if st.checkbox("å‚è€ƒæ–‡ç« ", key=idx):
                st.code(chunk[:2000])
